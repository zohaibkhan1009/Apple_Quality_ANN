import pandas as pd
import numpy as np
import seaborn as sns
import os
import pylab as pl
import matplotlib.pyplot as plt

os.chdir("C:\\Users\\zohaib khan\\OneDrive\\Desktop\\USE ME\\dump\\zk")

data = pd.read_csv("apple_quality.csv")

data.head()

pd.set_option('display.max_columns', None)

data.head()

#Check for missing values
data.isnull().sum()

from sklearn.impute import SimpleImputer

imputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent',fill_value=None)

data["Arrival Delay in Minutes"].fillna("mean", inplace=True)

#Check for duplicate values
data[data.duplicated()]

#remove duplicates permanent
data.drop_duplicates(inplace=True)



from autoviz.AutoViz_Class import AutoViz_Class 
AV = AutoViz_Class()
import matplotlib.pyplot as plt
%matplotlib INLINE
filename = 'apple_quality.csv'
sep =","
dft = AV.AutoViz(
    filename  
)



def class_distribution(data, column_name='Quality'):
    # Display total counts and percentage for each class
    distribution = data[column_name].value_counts()
    percentage = data[column_name].value_counts(normalize=True) * 100
    
    print(f"Class distribution for '{column_name}':")
    print(distribution)
    print("\nPercentage distribution:")
    print(percentage)

# Call the function to display the distribution for the 'Resigned' column
class_distribution(data, 'Quality')


# To show Outliers in the data set run the code 

num_vars = data.select_dtypes(include=['int','float']).columns.tolist()

num_cols = len(num_vars)
num_rows = (num_cols + 2 ) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5*num_rows))
axs = axs.flatten()

for i, var in enumerate (num_vars):
    sns.boxplot(x=data[var],ax=axs[i])
    axs[i].set_title(var)

if num_cols < len(axs):
  for i in range(num_cols , len(axs)):
    fig.delaxes(axs[i])

plt.tight_layout()
plt.show()



def pintu (data,age):
 Q1 = data[age].quantile(0.25)
 Q3 = data[age].quantile(0.75)
 IQR = Q3 - Q1
 data= data.loc[~((data[age] < (Q1 - 1.5 * IQR)) | (data[age] > (Q3 + 1.5 * IQR))),]
 return data

data.boxplot(column=["Acidity"])

data = pintu(data,"Acidity")

data.info()


from sklearn import preprocessing
for col in data.select_dtypes(include=['object']).columns:
    label_encoder = preprocessing.LabelEncoder()
    label_encoder.fit(data[col].unique())
    data[col] = label_encoder.transform(data[col])
    print(f'{col} : {data[col].unique()}')



#Segregrating dataset into X and y

X = data.drop("Quality", axis = 1)

y = data["Quality"]

X.head()

y.head()

#Splitting the dataset into testing and training

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)


#Scale the dataset
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)



# machine learning libraries
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier

# initialize classifiers
log_model = LogisticRegression()
rf_model = RandomForestClassifier()
svm_model = SVC()
knn_model = KNeighborsClassifier()
gnb_model = GaussianNB()

# logistic regression model fitting
log_model.fit(X_train, y_train)

# random forest classifier model fitting
rf_model.fit(X_train, y_train)

# k nearest neighbour model fitting
knn_model.fit(X_train, y_train)

# gaussian naive bayes model fitting
gnb_model.fit(X_train, y_train)



# model testing libraries
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


# logistic regression model testing
y_pred_log = log_model.predict(X_test)
print("Logistic Regression")
print(f"Accuracy: {accuracy_score(y_test, y_pred_log)}")
print(f"Precision: {precision_score(y_test, y_pred_log, average='weighted')}")
print(f"Recall: {recall_score(y_test, y_pred_log, average='weighted')}")
print(f"F1 score: {f1_score(y_test, y_pred_log, average='weighted')}")


# random forest classifier model testing
y_pred_rf = rf_model.predict(X_test)
print("Random Forest Classifier")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf)}")
print(f"Precision: {precision_score(y_test, y_pred_rf, average='weighted')}")
print(f"Recall: {recall_score(y_test, y_pred_rf, average='weighted')}")
print(f"F1 score: {f1_score(y_test, y_pred_rf, average='weighted')}")


# support vector classifier model testing
y_pred_svm = svm_model.predict(X_test)
print("Support Vector Classifier")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm)}")
print(f"Precision: {precision_score(y_test, y_pred_svm, average='weighted')}")
print(f"Recall: {recall_score(y_test, y_pred_svm, average='weighted')}")
print(f"F1 score: {f1_score(y_test, y_pred_svm, average='weighted')}")


# k nearest neighbour model testing
y_pred_knn = knn_model.predict(X_test)
print("K Nearest Neighbours")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn)}")
print(f"Precision: {precision_score(y_test, y_pred_knn, average='weighted')}")
print(f"Recall: {recall_score(y_test, y_pred_knn, average='weighted')}")
print(f"F1 score: {f1_score(y_test, y_pred_knn, average='weighted')}")


# gaussian naive bayes model testing
y_pred_gnb = gnb_model.predict(X_test)
print("Gaussian Naive Bayes")
print(f"Accuracy: {accuracy_score(y_test, y_pred_gnb)}")
print(f"Precision: {precision_score(y_test, y_pred_gnb, average='weighted')}")
print(f"Recall: {recall_score(y_test, y_pred_gnb, average='weighted')}")
print(f"F1 score: {f1_score(y_test, y_pred_gnb, average='weighted')}")



n_est=100
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=n_est, n_jobs=-1, random_state=0)
rfc.fit(X_train, y_train);

print("Training data accuracy:", rfc.score(X_train, y_train))
print("Testing data accuracy", rfc.score(X_test, y_test)) 

from sklearn.metrics import accuracy_score
predictions = rfc.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(str(np.round(acc*100, 2))+'%')


#XGboost with feature importance

from xgboost import XGBClassifier
from matplotlib import pyplot
from xgboost import plot_importance
xgbc = XGBClassifier(tree_method='auto', n_estimators=n_est, n_jobs=-1, random_state=0)
xgbc.fit(X_train, y_train);
plot_importance(xgbc)
pyplot.show()

print("Training data accuracy:", xgbc.score(X_train, y_train))
print("Testing data accuracy", xgbc.score(X_test, y_test))

from sklearn.metrics import accuracy_score

predictions = xgbc.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(str(np.round(acc*100, 2))+'%')



#XGBOOST Classifier with accuracy score

from xgboost import XGBClassifier

xgbc = XGBClassifier(tree_method='auto', n_estimators=n_est, n_jobs=-1, random_state=0)
xgbc.fit(X_train, y_train);

print("Training data accuracy:", xgbc.score(X_train, y_train))
print("Testing data accuracy", xgbc.score(X_test, y_test))

from sklearn.metrics import accuracy_score
predictions = xgbc.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(str(np.round(acc*100, 2))+'%')


#Run Sklearn Gradient Boost model classifier


from sklearn.ensemble import GradientBoostingClassifier

sgbc = GradientBoostingClassifier(n_estimators=n_est, random_state=0)
sgbc.fit(X_train, y_train);

print("Training data accuracy:", sgbc.score(X_train, y_train))
print("Testing data accuracy", sgbc.score(X_test, y_test))




#Run ADABOOST

from sklearn.ensemble import AdaBoostClassifier

adc = AdaBoostClassifier(n_estimators=n_est, random_state=0)
adc.fit(X_train, y_train);

print("Training data accuracy:", adc.score(X_train, y_train))
print("Testing data accuracy", adc.score(X_test, y_test))




#Run Bagging Classifier

from sklearn.ensemble import BaggingClassifier

bc = BaggingClassifier(n_estimators=n_est, random_state=0)
bc.fit(X_train, y_train);


print("Training data accuracy:", bc.score(X_train, y_train))
print("Testing data accuracy", bc.score(X_test, y_test))




import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import datetime
import tensorflow as tf
import tensorflow as tf
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras import backend  


Loading the ANN
classifier = Sequential()


classifier.add(Dense(units=4, 
kernel_initializer='uniform', activation='relu',
input_dim=7))

classifier.add(Dense(units=4, 
kernel_initializer='uniform', activation='relu'))

classifier.add(Dense(units=4, 
kernel_initializer='uniform', activation='relu'))

#Adding the Output Layer
classifier.add(Dense(units=1, 
kernel_initializer='uniform', activation='sigmoid'))

classifier.compile(optimizer='adam', 
loss='binary_crossentropy', metrics=['accuracy'])


classifier.fit(X_train, y_train, batch_size=10,
epochs=50, validation_split=0.1)



#Predicting the Model

y_pred = classifier.predict(X_test)
y_pred = np.where(y_pred > 0.5,1,0)



#Make the confusion Matrix

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
cm


#calculate accuracy

from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,y_test)
score
